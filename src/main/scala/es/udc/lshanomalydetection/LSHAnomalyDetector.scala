package es.udc.lshanomalydetection

import java.io.File
import java.io.PrintWriter

import org.apache.log4j.Level
import org.apache.log4j.Logger
import org.apache.spark.HashPartitioner
import org.apache.spark.internal.Logging
import org.apache.spark.ml.PredictionModel
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.linalg.{ Vectors => MLVectors }
import org.apache.spark.ml.param.Param
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.ml.param.Params
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
//import es.udc.graph.utils.GraphUtils
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

import es.udc.graph.EuclideanLSHasher
import es.udc.graph.EuclideanLSHasherForAnomaly
import es.udc.graph.Hash
import es.udc.graph.Hasher
import es.udc.graph.KNiNe
import es.udc.graph.LSHKNNGraphBuilder
import es.udc.graph.sparkContextSingleton
import vegas.AggOps
import vegas.Bar
import vegas.Bin
import vegas.DefaultValueTransformer
import vegas.Quantitative
import vegas.Vegas
import org.apache.spark.mllib.feature.StandardScaler

trait LSHAnomalyDetectorParams extends Params
{
  final val minBucketSize= new Param[Int](this, "desiredSize", "Minimum size of the buckets generated")
  final val numTablesMultiplier= new Param[Int](this, "numTablesMultiplier", "Multiplying factor for the autogenerated numTables")
  final val anomalyValue= new Param[Double](this, "anomalyValue", "Label used to identify an anomaly in the training dataset")
  final val numPartitions= new Param[Int](this, "numPartitions", "Number of partitions to use")
  //DEBUG
  final val histogramFilePath= new Param[Option[String]](this, "histogramFilePath", "DEBUG - Path to save the histograms")
}

class LSHAnomalyDetectorModel(private val hashCounts:scala.collection.Map[Hash,Int], private val hasher:Hasher, private val threshold:Int, private val radius:Double)
  extends PredictionModel[Vector, LSHAnomalyDetectorModel]
  with LSHAnomalyDetectorParams with Serializable
{
  def predict(features:Vector):Double=
  {
    val hashes=hasher.getHashes(Vectors.dense(features.toArray), -1, radius)
    if (hashes.map({case (h,id) => hashCounts.getOrElse(h, 0)}).sum<=threshold)
      return 1.0
    return 0.0
  }
  
  def getEstimator(features:Vector):Double=
  {
    val hashes=hasher.getHashes(Vectors.dense(features.toArray), -1, radius)
    return -hashes.map({case (h,id) => hashCounts.getOrElse(h, 0)}).sum
  }
  val uid: String = Identifiable.randomUID("LSHAnomalyDetectorModel")
  override def copy(extra:ParamMap): LSHAnomalyDetectorModel = defaultCopy(extra)
}

class LSHAnomalyDetector(override val uid: String)
  extends LSHAnomalyDetectorParams
  with Logging
{
  def this() = this(Identifiable.randomUID("LSHAnomalyDetector"))
  override def copy(extra:ParamMap): LSHAnomalyDetector = defaultCopy(extra)
  
  def setMinBucketSize(v:Int):this.type=set(minBucketSize, v)
  setDefault(minBucketSize, 50)
  def setAnomalyValue(v:Double):this.type=set(anomalyValue, v)
  setDefault(anomalyValue, 1.0)
  def setNumPartitions(v:Int):this.type=set(numPartitions, v)
  setDefault(numPartitions, 512)
  def setHistogramFilePath(v:Option[String]):this.type=set(histogramFilePath, v)
  setDefault(histogramFilePath, None)
  def setNumTablesMultiplier(v:Int):this.type=set(numTablesMultiplier, v)
  setDefault(numTablesMultiplier, 100)
  
  protected def fit(rddData: RDD[LabeledPoint]): LSHAnomalyDetectorModel=
  {
    
    val dataRDD=rddData.zipWithIndex()
                        .map(_.swap)
                        .partitionBy(new HashPartitioner($(numPartitions)))
    
    val desiredSize=$(minBucketSize)
    val ANOMALY_VALUE = $(anomalyValue)
    
    //Make sure the number of anomalies is around 1%
    val dataNumElems=dataRDD.count()
    val dataNumAnomalies=dataRDD.filter({case (id, point) => point.label==ANOMALY_VALUE}).count()
    
    val (trainingDataRDD,numElems,numAnomalies)=
      if (dataNumAnomalies.toDouble/dataNumElems<=0.02)
        (dataRDD,dataNumElems,dataNumAnomalies)
      else
      {
        val tr=dataRDD.filter({case (id, point) => point.label!=ANOMALY_VALUE || (math.random<0.01*dataNumElems.toDouble/dataNumAnomalies)})
        (tr,tr.count(),tr.filter({case (id, point) => point.label==ANOMALY_VALUE}).count())
      }
    
    logDebug(s"Tuning hasher with desiredSize=$desiredSize...")
    //Get a new hasher
    //Autoconfig
    val (hasher,nComps,suggestedRadius)=EuclideanLSHasherForAnomaly.getHasherForDataset(trainingDataRDD, desiredSize) //Make constant size buckets
    //Quick Parameters
    //val hasher = new EuclideanLSHasher(dataRDD.first()._2.features.size, keyLength, numTables)
    
    logDebug(s"Autotuned params:\n\tKL:${hasher.keyLength}\n\tR0:$suggestedRadius\n\tNT:${hasher.numTables}")
    
    logDebug("Training...")
    val newHasher = new EuclideanLSHasher(trainingDataRDD.first()._2.features.size, hasher.keyLength, 100*hasher.numTables)
    
    //val hashNeighborsRDD = EuclideanLSHasherForAnomaly.getHashNeighbors(trainingDataRDD, newHasher, suggestedRadius) // ((a,b,c), (b,d), (c), (a,h,f,e) (a))
    val hashedDataRDD=EuclideanLSHasherForAnomaly.hashData(trainingDataRDD, newHasher, suggestedRadius)
                                                 .groupByKey()
    hashedDataRDD.cache()
    val hashNeighborsRDD=hashedDataRDD.map(_._2)
   
    if ($(histogramFilePath).isDefined)
    {
      //DEBUG PLOTTING
        val histogramPath=$(histogramFilePath).get
        val plotGeneral=Vegas("A simple bar chart with embedded data.").
          withData(hashNeighborsRDD.map({case l => Map("a" -> l.size)}).collect().toSeq).
          encodeX("a", Quantitative, bin=Bin(maxbins=20.0)).
          encodeY(field="*", Quantitative, aggregate=AggOps.Count).
          mark(Bar)
          
        val normalID=trainingDataRDD.filter(_._2.label!=ANOMALY_VALUE).first()._1
        val plotNormal=Vegas("A simple bar chart with embedded data.").
          withData(hashNeighborsRDD.filter(_.toSet.contains(normalID)).map({case l => Map("a" -> l.size)}).collect().toSeq).
          encodeX("a", Quantitative, bin=Bin(maxbins=20.0)).
          encodeY(field="*", Quantitative, aggregate=AggOps.Count).
          mark(Bar)
          
        val anomalyID=trainingDataRDD.filter(_._2.label==ANOMALY_VALUE).first()._1
        val plotAnomaly=Vegas("A simple bar chart with embedded data.").
          withData(hashNeighborsRDD.filter(_.toSet.contains(anomalyID)).map({case l => Map("a" -> l.size)}).collect().toSeq).
          encodeX("a", Quantitative, bin=Bin(maxbins=20.0)).
          encodeY(field="*", Quantitative, aggregate=AggOps.Count).
          mark(Bar)
        
        val pw = new PrintWriter(new File(histogramPath))
        pw.write(plotGeneral.html.headerHTML(""))
        pw.write(plotGeneral.html.plotHTML("general"))
        pw.write(plotNormal.html.plotHTML("normal"))
        pw.write(plotAnomaly.html.plotHTML("anomaly"))
        pw.write(plotGeneral.html.footerHTML)
        pw.close
      //END OF DEBUG PLOTTING
    }
    
    val numNeighborsPerPointRDD = hashNeighborsRDD.flatMap({case l => l.map({case x =>(x, l.size-1)})})
                                                  .reduceByKey(_ + _)
                                                  //.sortBy(_._2) //No need to sort the entire RDD here
                                                  //.partitionBy(new HashPartitioner(numPartitions))
    
    //Retrieve the N=numAnomalies elements with fewer neighbors. They will constitute the predicted anomalies. The largest number of neighbors is the threshold.
    val maxNeighborsForAnomaly = numNeighborsPerPointRDD.map(_._2).takeOrdered(numAnomalies.toInt).last
    
    
    new LSHAnomalyDetectorModel(hashedDataRDD.map({case (hash,neighbors) => (hash,neighbors.size)}).collectAsMap(),
                                newHasher,
                                maxNeighborsForAnomaly,
                                suggestedRadius)
  }
}

object LSHAnomalyDetector
{
  val DEFAULT_NUM_PARTITIONS:Double=512
  val DEFAULT_THRESHOLD:Int=1
  val ANOMALY_VALUE=1.0
  
  def showUsageAndExit()=
  {
    println("""teste Usage: LSHAnomalyDetector dataset [options]
    Dataset must be a libsvm file
Options:
    -r    Starting radius (default: """+LSHKNNGraphBuilder.DEFAULT_RADIUS_START+""")
    -p    Number of partitions for the data RDDs (default: """+DEFAULT_NUM_PARTITIONS+""")
    -f    threshold value in % (default: """+DEFAULT_THRESHOLD+""")

Advanced LSH options:
    -n    Number of hashes per item (default: auto)
    -l    Hash length (default: auto)""")
    System.exit(-1)
  }
  def parseParams(p:Array[String]):Map[String, Any]=
  {
    val m=scala.collection.mutable.Map[String, Any]("radius_start" -> LSHKNNGraphBuilder.DEFAULT_RADIUS_START,
                                                    "num_partitions" -> KNiNe.DEFAULT_NUM_PARTITIONS,
                                                    "threshold" -> DEFAULT_THRESHOLD.toDouble)
    if (p.length<=0)
      showUsageAndExit()
    
    m("dataset")=p(0)
    
    var i=1
    while (i < p.length)
    {
      if ((i>=p.length-1) || (p(i).charAt(0)!='-'))
      {
        println("Unknown option: "+p(i))
        showUsageAndExit()
      }
      val readOptionName=p(i).substring(1)
      val option=readOptionName match
        {
          case "r"   => "radius_start"
          case "n"   => "num_tables"
          case "l"   => "key_length"
          case "p"   => "num_partitions"
          case "f"   => "threshold"
          case somethingElse => readOptionName
        }
      if (!m.keySet.exists(_==option) && option==readOptionName)
      {
        println("Unknown option:"+readOptionName)
        showUsageAndExit()
      }
      m(option)=p(i+1).toDouble
      
      i=i+2
    }
    return m.toMap
  }
  
  def evaluateModel(model:LSHAnomalyDetectorModel, testDataRDD:RDD[LabeledPoint]):Double=
  {
    val bModel=testDataRDD.sparkContext.broadcast(model)
    val checkRDD = testDataRDD.map(
                                    {
                                      case lp =>
                                        val m=bModel.value
                                        val f=MLVectors.dense(lp.features.toArray)
                                        (m.predict(f)>0,m.getEstimator(f),lp.label==ANOMALY_VALUE)
                                    })
                                    
    val confMat =  checkRDD.map(
                                  {
                                    case (pred,estimator,label) => 
                                          var vp=if(label && pred) 1 else 0
                                          var vn=if(!label && !pred) 1 else 0
                                          var fp=if(!label && pred) 1 else 0
                                          var fn=if(label && !pred) 1 else 0 
                                          (vp,vn,fp,fn)
                                  })
                              .reduce(
                                  {
                                    case ((tp1,tn1,fp1,fn1), (tp2,tn2,fp2,fn2)) =>
                                          (tp1+tp2, tn1+tn2, fp1+fp2, fn1+fn2)
                                  })
      val tp =confMat._1.toFloat
      val tn =confMat._2.toFloat
      val fp =confMat._3.toFloat
      val fn =confMat._4.toFloat
      
      val accuracy = (tp+tn)/(tp+tn+fp+fn)
      val precision = (tp)/(tp+fp)
      val recall = (tp)/(tp+fn)
      val f1score = 2*((precision*recall)/(precision+recall))
      val metricsForEstimation = new BinaryClassificationMetrics(checkRDD.map({case (pred,estimator,label) => (estimator, if (label) 1.toDouble else 0.toDouble)}))
      val metricsForPrediction = new BinaryClassificationMetrics(checkRDD.map({case (pred,estimator,label) => (if (pred) 1.toDouble else 0.toDouble, if (label) 1.toDouble else 0.toDouble)}))
      // AUROC
      val auROCForEstimation = metricsForEstimation.areaUnderROC
      val auROCForPrediction = metricsForPrediction.areaUnderROC
      
      //print("pointsPerHash")
      //numNeighborsPerPointRDD.take(100).foreach(println)
      
      println("\tconfMatTuple: "+confMat)
      println("\taccuracy: "+accuracy)
      println("\tprecision: "+precision)
      println("\trecall: "+recall)
      println("\tf1score: "+f1score)
      println("\tArea under ROC (estimation)= " + auROCForEstimation)
      println("\tArea under ROC (prediction)= " + auROCForPrediction)
      
      return auROCForEstimation
  }
  
  def main(args: Array[String])
  {
    //println("JM-> args: "+args(0))
    if (args.length <= 0)
    {
      showUsageAndExit()
      return
    }
    
    val options=parseParams(args)
    
    val datasetFile=options("dataset").asInstanceOf[String]
    
    val threshold = options("threshold").asInstanceOf[Double].toInt
    val threshold_per = threshold/100f
    
    val numPartitions=options("num_partitions").asInstanceOf[Double].toInt
    val paramRadius = options("radius_start").asInstanceOf[Double]
    //Set up Spark Context
    val sc=sparkContextSingleton.getInstance()
    println(s"Default parallelism: ${sc.defaultParallelism}")
    
    //Stop annoying INFO messages
    val rootLogger = Logger.getRootLogger()
    rootLogger.setLevel(Level.WARN)
    
    val DATASETS_ROOT="file:///mnt/NTFS/owncloud/Datasets/datasets-anomalias/"
    val NUM_FOLDS=5
    
    val pw = new PrintWriter(new File("/home/eirasf/Escritorio/summary.txt"))
    
    for (datasetName <- Array("abalone1-8","abalone9-11","abalone11-29","arritmia","german_statlog"))
    {
      val dataRDD: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, DATASETS_ROOT+datasetName+".libsvm")
      
      val scaler = new StandardScaler(withMean = true, withStd = true).fit(dataRDD.map(x => x.features))
      val standardDataRDD=dataRDD.map({case p => new LabeledPoint(p.label,scaler.transform(p.features))})
      
      val folds=MLUtils.kFold(dataRDD, NUM_FOLDS, System.nanoTime().toInt)
      
      for (mf <- Array(5,10,100))
        for (bs <- Array(5,10,100))
        {
          var i=0
          var totalAUC=0.0
          for (f <- folds)
          {
            i=i+1
            val trainDataRDD=f._1
            val testDataRDD=f._2
            
            println(s">>>> $datasetName - Fold #$i - MinBucketSize:$bs - Multiplying factor:$mf")
            try
            {
              val model=new LSHAnomalyDetector()
                            .setMinBucketSize(bs)
                            .setNumTablesMultiplier(mf)
                            .setHistogramFilePath(Some(s"/home/eirasf/Escritorio/$datasetName-$bs-$mf.html"))
                            .fit(trainDataRDD)
                            
              
              totalAUC+=evaluateModel(model,testDataRDD)
            }catch
            {
              case e : Exception =>
                println("ERROR")
            }
          }
          println(s"----------------------------------\n$datasetName - MinBucketSize:$bs - Multiplying factor:$mf - Avg. AUROC=${totalAUC/NUM_FOLDS}\n\n\n\n")
          pw.write(s"$datasetName - MinBucketSize:$bs - Multiplying factor:$mf - Avg. AUROC=${totalAUC/NUM_FOLDS}\n")
          pw.flush()
        }
    }
    pw.close

    //Stop the Spark Context
    sc.stop()
  }
}